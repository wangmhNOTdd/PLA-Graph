#!/usr/bin/env python3
"""
DSANæ‰¹é‡è®­ç»ƒæµ‹è¯•è„šæœ¬ - éªŒè¯batch_size=4æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""

import torch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('.')

from models.DSAN.encoder import DSANEncoder, DSANLayer
from models import create_model

def test_batch_processing():
    """æµ‹è¯•DSANçš„æ‰¹é‡å¤„ç†èƒ½åŠ›"""
    print("ğŸ§ª æµ‹è¯•DSANæ‰¹é‡å¤„ç†ï¼ˆbatch_size=4ï¼‰")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿè®­ç»ƒå‚æ•°
    class Args:
        task = 'PDBBind'
        noisy_sigma = 0.0
        model_type = 'DSAN'
        hidden_size = 128
        n_channel = 1
        n_rbf = 16
        cutoff = 8.0
        radial_size = 8
        k_neighbors = 6
        n_layers = 2  # å‡å°‘å±‚æ•°åŠ å¿«æµ‹è¯•
        n_head = 8
        atom_level = False
        hierarchical = False
        no_block_embedding = False
        pretrain_ckpt = None
    
    args = Args()
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = create_model(args)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {device}")
        
        # åˆ›å»ºæ‰¹é‡æµ‹è¯•æ•°æ® (batch_size=4)
        batch_size = 4
        max_atoms_per_graph = 100
        max_blocks_per_graph = 20
        n_channel = 1
        
        # æ¨¡æ‹Ÿä¸åŒå¤§å°çš„åˆ†å­
        molecule_sizes = [80, 95, 60, 75]  # ä¸åŒåˆ†å­çš„åŸå­æ•°
        block_sizes = [16, 19, 12, 15]     # ä¸åŒåˆ†å­çš„å—æ•°
        
        # æ„å»ºæ‰¹é‡æ•°æ®
        all_atom_features = []
        all_positions = []
        all_block_features = []
        all_atom_positions = []
        all_block_lengths = []
        all_lengths = []
        all_segment_ids = []
        
        atom_offset = 0
        block_offset = 0
        
        for batch_idx in range(batch_size):
            n_atoms = molecule_sizes[batch_idx]
            n_blocks = block_sizes[batch_idx]
            
            # åŸå­ç‰¹å¾å’Œåæ ‡
            Z = torch.randn(n_atoms, n_channel, 3, device=device)
            B = torch.randint(0, 10, (n_blocks,), device=device)  # å—ç±»å‹
            A = torch.randint(0, 10, (n_atoms,), device=device)  # åŸå­ç±»å‹
            atom_positions = torch.randn(n_atoms, 3, device=device)
            
            # å—é•¿åº¦ï¼ˆæ¯ä¸ªå—åŒ…å«çš„åŸå­æ•°ï¼‰
            atoms_per_block = n_atoms // n_blocks
            block_lengths_single = torch.full((n_blocks,), atoms_per_block, device=device)
            # è°ƒæ•´æœ€åä¸€ä¸ªå—çš„é•¿åº¦
            remaining_atoms = n_atoms - (atoms_per_block * n_blocks)
            if remaining_atoms > 0:
                block_lengths_single[-1] += remaining_atoms
            
            all_atom_features.append(Z)
            all_positions.append(Z)
            all_block_features.append(B)
            all_atom_positions.append(atom_positions)
            all_block_lengths.append(block_lengths_single)
            all_lengths.append(torch.tensor([n_blocks], device=device))
            all_segment_ids.append(torch.full((n_blocks,), batch_idx, device=device))
        
        # åˆå¹¶æ‰¹é‡æ•°æ®
        Z_batch = torch.cat(all_atom_features, dim=0)  # [total_atoms, n_channel, 3]
        B_batch = torch.cat(all_block_features, dim=0)  # [total_blocks]
        A_batch = torch.cat([torch.randint(0, 10, (mol_size,), device=device) 
                            for mol_size in molecule_sizes], dim=0)  # [total_atoms]
        atom_positions_batch = torch.cat(all_atom_positions, dim=0)  # [total_atoms, 3]
        block_lengths_batch = torch.cat(all_block_lengths, dim=0)  # [total_blocks]
        lengths_batch = torch.tensor([sum(block_sizes[:i+1]) for i in range(batch_size)], device=device)
        segment_ids_batch = torch.cat(all_segment_ids, dim=0)  # [total_blocks]
        
        print(f"æ‰¹é‡æ•°æ®å½¢çŠ¶:")
        print(f"   Z: {Z_batch.shape}")
        print(f"   B: {B_batch.shape}")
        print(f"   A: {A_batch.shape}")
        print(f"   atom_positions: {atom_positions_batch.shape}")
        print(f"   block_lengths: {block_lengths_batch.shape}")
        print(f"   lengths: {lengths_batch.shape}")
        print(f"   segment_ids: {segment_ids_batch.shape}")
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆäº²å’ŒåŠ›ï¼‰
        label = torch.randn(batch_size, device=device)
        
        print("ğŸ”„ æ‰§è¡Œå‰å‘ä¼ æ’­...")
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():  # èŠ‚çœå†…å­˜
            try:
                loss = model(
                    Z=Z_batch,
                    B=B_batch, 
                    A=A_batch,
                    atom_positions=atom_positions_batch,
                    block_lengths=block_lengths_batch,
                    lengths=lengths_batch,
                    segment_ids=segment_ids_batch,
                    label=label
                )
                
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
                print(f"   æŸå¤±å€¼: {loss.item():.6f}")
                print(f"   æŸå¤±å½¢çŠ¶: {loss.shape}")
                
                return True
                
            except RuntimeError as e:
                if "illegal memory access" in str(e).lower():
                    print(f"âŒ CUDAéæ³•å†…å­˜è®¿é—®é”™è¯¯: {e}")
                    print("è¿™è¡¨æ˜ä»å­˜åœ¨ç´¢å¼•è¶Šç•Œé—®é¢˜")
                    return False
                else:
                    print(f"âŒ å…¶ä»–è¿è¡Œæ—¶é”™è¯¯: {e}")
                    return False
            except Exception as e:
                print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è®¾ç½®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_memory_status():
    """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
    if torch.cuda.is_available():
        print(f"\nğŸ’¾ å†…å­˜çŠ¶æ€:")
        print(f"   å·²åˆ†é…æ˜¾å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        print(f"   å·²é¢„ç•™æ˜¾å­˜: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   æ€»æ˜¾å­˜: {total_memory:.1f} GB")
        usage_percent = (torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory) * 100
        print(f"   æ˜¾å­˜åˆ©ç”¨ç‡: {usage_percent:.1f}%")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ DSANæ‰¹é‡å¤„ç†æµ‹è¯•")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    
    # åˆå§‹å†…å­˜çŠ¶æ€
    check_memory_status()
    
    # æ‰§è¡Œæµ‹è¯•
    success = test_batch_processing()
    
    # æ¸…ç†å†…å­˜
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # æœ€ç»ˆå†…å­˜çŠ¶æ€
    check_memory_status()
    
    print("=" * 60)
    if success:
        print("ğŸ‰ æ‰¹é‡å¤„ç†æµ‹è¯•æˆåŠŸï¼å¯ä»¥ä½¿ç”¨batch_size=4")
        print("ç°åœ¨å¯ä»¥æ”¾å¿ƒåœ°è¿›è¡Œæ‰¹é‡è®­ç»ƒ")
        return 0
    else:
        print("âš ï¸ æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        print("å»ºè®®ç»§ç»­ä½¿ç”¨batch_size=1æˆ–å°è¯•æ›´å°çš„æ‰¹æ¬¡")
        return 1

if __name__ == "__main__":
    exit(main())
