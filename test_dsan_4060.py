#!/usr/bin/env python3
"""
RTX 4060æ˜¾å­˜å‹åŠ›æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¼˜åŒ–åçš„DSANæ¨¡å‹åœ¨4060ä¸Šçš„æ˜¾å­˜å ç”¨
"""

import torch
import torch.nn as nn
import time
import psutil
import numpy as np
from models.DSAN.encoder import DSANEncoder


def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("ğŸ”§ GPUçŠ¶æ€æ£€æŸ¥:")
    if torch.cuda.is_available():
        print(f"   âœ… CUDAå¯ç”¨")
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   æ˜¾å­˜æ€»é‡: {gpu_memory:.1f} GB")
        print(f"   å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        return True, gpu_memory
    else:
        print("   âŒ CUDAä¸å¯ç”¨")
        return False, 0


def memory_monitor():
    """å®æ—¶æ˜¾å­˜ç›‘æ§"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        free = (torch.cuda.get_device_properties(0).total_memory / 1024**3) - reserved
        
        status = f"GPU: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved, {free:.2f}GB free"
        
        # 4060è­¦å‘Šé˜ˆå€¼
        if allocated > 6.0:
            status += " âš ï¸  é«˜æ˜¾å­˜ä½¿ç”¨!"
        if allocated > 7.0:
            status += " ğŸš¨ ä¸´ç•ŒçŠ¶æ€!"
            
        return status, allocated
    return "CUDAä¸å¯ç”¨", 0


def create_test_data(n_atoms=100, n_blocks=10, device='cuda'):
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    print(f"ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®: {n_atoms}ä¸ªåŸå­, {n_blocks}ä¸ªå—")
    
    # åŸå­ç‰¹å¾
    H = torch.randn(n_atoms, 64, device=device)  # å‡å°‘åˆ°64ç»´
    
    # åŸå­åæ ‡
    Z = torch.randn(n_atoms, 1, 3, device=device)
    
    # å—ID (æ¯ä¸ªå—å¤§çº¦10ä¸ªåŸå­)
    block_id = torch.randint(0, n_blocks, (n_atoms,), device=device)
    
    # æ‰¹æ¬¡ID
    batch_id = torch.zeros(n_blocks, device=device, dtype=torch.long)
    
    # å—é—´è¾¹ï¼ˆéšæœºç”Ÿæˆï¼‰
    n_edges = min(n_blocks * 2, 50)  # å‡å°‘è¾¹æ•°é‡
    edge_src = torch.randint(0, n_blocks, (n_edges,), device=device)
    edge_dst = torch.randint(0, n_blocks, (n_edges,), device=device)
    edges = torch.stack([edge_src, edge_dst], dim=0)
    
    return H, Z, block_id, batch_id, edges


def test_dsan_memory_usage():
    """æµ‹è¯•DSANæ¨¡å‹æ˜¾å­˜ä½¿ç”¨"""
    print("ğŸ§ª å¼€å§‹DSANæ˜¾å­˜æµ‹è¯•...")
    print("=" * 60)
    
    # æ£€æŸ¥GPU
    gpu_available, total_memory = check_gpu_status()
    if not gpu_available:
        return False
    
    device = 'cuda'
    torch.cuda.empty_cache()
    
    print("\nğŸ’¾ åŸºå‡†æ˜¾å­˜çŠ¶æ€:")
    baseline_status, baseline_memory = memory_monitor()
    print(f"   {baseline_status}")
    
    try:
        # åˆ›å»ºä¼˜åŒ–çš„DSANæ¨¡å‹ (4060å‚æ•°)
        print("\nğŸ—ï¸  åˆ›å»ºDSANæ¨¡å‹ (4060ä¼˜åŒ–å‚æ•°):")
        model = DSANEncoder(
            hidden_size=64,      # å‡å°‘éšè—å±‚å¤§å°
            n_layers=2,          # å‡å°‘å±‚æ•°  
            num_heads=4,         # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            k_neighbors=4,       # å‡å°‘Kè¿‘é‚»
            dropout=0.1,
            use_geometry=True,
            rbf_dim=8,           # å‡å°‘RBFç»´åº¦
            cutoff=6.0,          # å‡å°‘cutoff
            memory_efficient=True  # å¼€å¯æ˜¾å­˜ä¼˜åŒ–
        ).to(device)
        
        model_status, model_memory = memory_monitor()
        print(f"   æ¨¡å‹åŠ è½½å: {model_status}")
        print(f"   æ¨¡å‹å ç”¨æ˜¾å­˜: {model_memory - baseline_memory:.2f} GB")
        
        # æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ•°æ®
        test_cases = [
            (50, 5, "å°è§„æ¨¡"),
            (100, 10, "ä¸­ç­‰è§„æ¨¡"),  
            (200, 20, "å¤§è§„æ¨¡"),
            (300, 30, "æé™è§„æ¨¡")
        ]
        
        max_memory = model_memory
        successful_cases = []
        
        for n_atoms, n_blocks, desc in test_cases:
            print(f"\nğŸ¯ æµ‹è¯• {desc} ({n_atoms}åŸå­, {n_blocks}å—):")
            
            try:
                # åˆ›å»ºæµ‹è¯•æ•°æ®
                H, Z, block_id, batch_id, edges = create_test_data(n_atoms, n_blocks, device)
                
                # å‰å‘ä¼ æ’­
                torch.cuda.empty_cache()
                start_time = time.time()
                
                with torch.no_grad():
                    atom_features, block_repr, graph_repr, pred_Z = model(H, Z, block_id, batch_id, edges)
                
                torch.cuda.synchronize()
                end_time = time.time()
                
                # ç›‘æ§æ˜¾å­˜
                test_status, test_memory = memory_monitor()
                inference_time = (end_time - start_time) * 1000
                
                print(f"   æ¨ç†æ—¶é—´: {inference_time:.1f} ms")
                print(f"   æ˜¾å­˜çŠ¶æ€: {test_status}")
                print(f"   å³°å€¼æ˜¾å­˜: {test_memory:.2f} GB")
                print(f"   è¾“å‡ºå½¢çŠ¶: atom_features{atom_features.shape}, block_repr{block_repr.shape}")
                
                if test_memory > max_memory:
                    max_memory = test_memory
                    
                successful_cases.append((desc, n_atoms, n_blocks, test_memory, inference_time))
                
                # æ¸…ç†
                del H, Z, block_id, batch_id, edges, atom_features, block_repr, graph_repr, pred_Z
                torch.cuda.empty_cache()
                
                print("   âœ… æµ‹è¯•é€šè¿‡")
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"   âŒ æ˜¾å­˜ä¸è¶³: {e}")
                    torch.cuda.empty_cache()
                    break
                else:
                    raise e
        
        # æµ‹è¯•æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æµ‹è¯•æ€»ç»“:")
        print(f"   GPUå‹å·: {torch.cuda.get_device_name(0)} ({total_memory:.1f}GB)")
        print(f"   åŸºå‡†æ˜¾å­˜: {baseline_memory:.2f} GB")
        print(f"   æ¨¡å‹æ˜¾å­˜: {model_memory - baseline_memory:.2f} GB") 
        print(f"   å³°å€¼æ˜¾å­˜: {max_memory:.2f} GB")
        print(f"   æ˜¾å­˜åˆ©ç”¨ç‡: {(max_memory / total_memory) * 100:.1f}%")
        
        print(f"\nğŸ‰ æˆåŠŸæµ‹è¯•çš„è§„æ¨¡:")
        for desc, n_atoms, n_blocks, memory, time_ms in successful_cases:
            print(f"   {desc}: {n_atoms}åŸå­/{n_blocks}å— -> {memory:.2f}GB, {time_ms:.1f}ms")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        # æ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()
        final_status, _ = memory_monitor()
        print(f"\nğŸ§¹ æ¸…ç†åçŠ¶æ€: {final_status}")


def main():
    print("ğŸš€ RTX 4060 DSANæ˜¾å­˜å‹åŠ›æµ‹è¯•")
    print("=" * 60)
    
    success = test_dsan_memory_usage()
    
    if success:
        print("\nâœ… æ˜¾å­˜æµ‹è¯•å®Œæˆ! DSANæ¨¡å‹å·²é’ˆå¯¹RTX 4060ä¼˜åŒ–")
        print("ğŸ’¡ å»ºè®®ä½¿ç”¨ä»¥ä¸‹è®­ç»ƒå‚æ•°:")
        print("   - batch_size: 1")
        print("   - hidden_size: 64")
        print("   - n_layers: 2")
        print("   - k_neighbors: 4")
        print("   - cutoff: 6.0")
        print("   - rbf_dim: 8")
        return 0
    else:
        print("\nâŒ æ˜¾å­˜æµ‹è¯•å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit(main())
