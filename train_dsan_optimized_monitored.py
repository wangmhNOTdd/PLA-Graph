#!/usr/bin/env python3
"""
DSANæ€§èƒ½ç›‘æ§è„šæœ¬
ç›‘æ§è®­ç»ƒæ—¶çš„GPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨å’Œè®­ç»ƒé€Ÿåº¦
"""

import subprocess
import sys
import time
import threading
import torch
import psutil
import os

class PerformanceMonitor:
    def __init__(self):
        self.monitoring = False
        self.stats = {
            'gpu_memory': [],
            'gpu_utilization': [],
            'cpu_usage': [],
            'ram_usage': [],
            'timestamps': []
        }
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
        
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1)
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                # è·å–å½“å‰æ—¶é—´
                current_time = time.time()
                
                # GPUå†…å­˜ä½¿ç”¨
                if torch.cuda.is_available():
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
                    # GPUä½¿ç”¨ç‡ï¼ˆè¿‘ä¼¼ï¼‰
                    gpu_util = torch.cuda.utilization() if hasattr(torch.cuda, 'utilization') else 0
                else:
                    gpu_memory = 0
                    gpu_util = 0
                
                # CPUå’ŒRAMä½¿ç”¨ç‡
                cpu_usage = psutil.cpu_percent()
                ram_usage = psutil.virtual_memory().percent
                
                # è®°å½•æ•°æ®
                self.stats['gpu_memory'].append(gpu_memory)
                self.stats['gpu_utilization'].append(gpu_util)
                self.stats['cpu_usage'].append(cpu_usage)
                self.stats['ram_usage'].append(ram_usage)
                self.stats['timestamps'].append(current_time)
                
                time.sleep(1)  # æ¯ç§’é‡‡æ ·ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§å‡ºé”™: {e}")
                time.sleep(1)
    
    def get_summary(self):
        """è·å–ç›‘æ§æ‘˜è¦"""
        if not self.stats['timestamps']:
            return "æ²¡æœ‰ç›‘æ§æ•°æ®"
        
        duration = self.stats['timestamps'][-1] - self.stats['timestamps'][0]
        
        summary = f"""
æ€§èƒ½ç›‘æ§æ‘˜è¦:
={'='*40}
ç›‘æ§æ—¶é•¿: {duration:.1f} ç§’
GPUå†…å­˜ä½¿ç”¨: å¹³å‡ {sum(self.stats['gpu_memory'])/len(self.stats['gpu_memory']):.2f} GB, å³°å€¼ {max(self.stats['gpu_memory']):.2f} GB
CPUä½¿ç”¨ç‡: å¹³å‡ {sum(self.stats['cpu_usage'])/len(self.stats['cpu_usage']):.1f}%
RAMä½¿ç”¨ç‡: å¹³å‡ {sum(self.stats['ram_usage'])/len(self.stats['ram_usage']):.1f}%
"""
        return summary

def main():
    print("ğŸš€ å¯åŠ¨DSANä¼˜åŒ–ç‰ˆæ¨¡å‹è®­ç»ƒ (å¸¦æ€§èƒ½ç›‘æ§)...")
    print("=" * 50)
    
    # æ£€æŸ¥GPUçŠ¶æ€
    print("GPUçŠ¶æ€æ£€æŸ¥:")
    import torch
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜å¤§å°: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print("=" * 50)
    
    # å¯åŠ¨æ€§èƒ½ç›‘æ§
    monitor = PerformanceMonitor()
    monitor.start_monitoring()
    
    print("æ€§èƒ½ç›‘æ§å·²å¯åŠ¨...")
    print("è®­ç»ƒå‚æ•°:")
    print(f"   æ¨¡å‹ç±»å‹: DSAN_Optimized (å‘é‡åŒ–ä¼˜åŒ–ç‰ˆ)")
    print(f"   æ•°æ®é›†: PDBBind identity30")
    print(f"   éšè—å±‚å¤§å°: 128")
    print(f"   å±‚æ•°: 3")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: 8")
    print(f"   å­¦ä¹ ç‡: 0.001 -> 0.0001")
    print(f"   æœ€å¤§è½®æ•°: 100")
    print(f"   æ‰¹æ¬¡å¤§å°: 16 (ä»8ä¼˜åŒ–)")
    print(f"   è®¾å¤‡: GPU-0")
    print(f"   ä¼˜åŒ–ç‰¹æ€§: å‘é‡åŒ–å—å¤„ç† + å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›")
    print("=" * 50)
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = [
        sys.executable, "train.py",
        "--train_set", "./datasets/PDBBind/processed/identity30/train.pkl",
        "--valid_set", "./datasets/PDBBind/processed/identity30/valid.pkl", 
        "--save_dir", "./datasets/PDBBind/processed/identity30/models/DSAN_optimized",
        "--task", "PDBBind",
        "--lr", "0.001",
        "--final_lr", "0.0001", 
        "--max_epoch", "100",
        "--save_topk", "5",
        "--batch_size", "16",  # ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        "--valid_batch_size", "16",
        "--grad_clip", "1.0",
        "--warmup", "1000",
        "--shuffle",
        "--model_type", "DSAN_Optimized",  # ä½¿ç”¨ä¼˜åŒ–ç‰ˆDSAN
        "--hidden_size", "128",
        "--n_layers", "3",
        "--n_channel", "1", 
        "--n_rbf", "32",
        "--cutoff", "10.0",
        "--n_head", "8",
        "--radial_size", "16",  # ä»16å‡å°‘åˆ°16ä¿æŒä¸å˜ï¼ˆä¹Ÿå¯ä»¥å‡å°‘åˆ°8ï¼‰
        "--k_neighbors", "9",
        "--seed", "2024",
        "--gpus", "0"
    ]
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # æ‰§è¡Œè®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    try:
        result = subprocess.run(cmd, check=True, capture_output=False)
        end_time = time.time()
        training_time = end_time - start_time
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: ./datasets/PDBBind/processed/identity30/models/DSAN_optimized")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {training_time/3600:.2f} å°æ—¶")
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        return_code = 1
    except KeyboardInterrupt:
        print("â›” è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return_code = 1
    else:
        return_code = 0
    finally:
        # åœæ­¢ç›‘æ§å¹¶æ˜¾ç¤ºæ‘˜è¦
        monitor.stop_monitoring()
        print("\n" + monitor.get_summary())
    
    return return_code

if __name__ == "__main__":
    exit(main())
