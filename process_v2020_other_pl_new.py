#!/usr/bin/env python3
"""
é‡æ–°å¤„ç†v2020-other-PLæ•°æ®é›†
1. ä»INDEX_general_PL.2020æ–‡ä»¶è¯»å–äº²å’ŒåŠ›æ•°æ®
2. è®­ç»ƒé›†ï¼šv2020-other-PLä¸­é™¤å»CASF-2016çš„æ•°æ®
3. éªŒè¯é›†ï¼šä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å–10%
4. æµ‹è¯•é›†ï¼šCASF-2016çš„æ‰€æœ‰285æ¡æ•°æ®
5. å¤„ç†åæ•°æ®ä¿å­˜åœ¨./datasets/v2020-other-PL/processed/
"""

import os
import pickle
import random
import pandas as pd
import re
import math
from pathlib import Path

def get_casf_2016_entries():
    """ä»CASF-2016/power_scoring/CoreSet.datè·å–æ‰€æœ‰æ¡ç›®"""
    casf_file = "./datasets/v2020-other-PL/CASF-2016/power_scoring/CoreSet.dat"
    casf_entries = set()
    
    print(f"Reading CASF-2016 entries from {casf_file}")
    with open(casf_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 1:
                    pdb_id = parts[0].lower()  # è½¬æ¢ä¸ºå°å†™ä»¥åŒ¹é…ç›®å½•å
                    casf_entries.add(pdb_id)
    
    print(f"Found {len(casf_entries)} CASF-2016 entries")
    return casf_entries

def parse_affinity_value(affinity_str):
    """è§£æäº²å’ŒåŠ›å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸º-log(Ka)å€¼ï¼Œåªä¿ç•™ç¡®å®šçš„å€¼"""
    try:
        # å»é™¤æ³¨é‡Šéƒ¨åˆ†
        if '//' in affinity_str:
            affinity_str = affinity_str.split('//')[0].strip()
        
        # è·³è¿‡æ‰€æœ‰ä¸ç¡®å®šçš„å€¼ï¼ˆåŒ…å«ä¸ç­‰å·æˆ–çº¦ç­‰å·ï¼‰
        if any(symbol in affinity_str for symbol in ['<', '>', '~', '<=', '>=']):
            return None
        
        # æ›´å¹¿æ³›çš„æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…Ki, Kd, IC50
        # æ”¯æŒæ›´å¤šæ•°å€¼æ ¼å¼ï¼ŒåŒ…æ‹¬ç§‘å­¦è®¡æ•°æ³•
        patterns = [
            r'Ki=([0-9.]+(?:[eE][+-]?[0-9]+)?)([upnfmM]?)M?',      # Kiå€¼
            r'Kd=([0-9.]+(?:[eE][+-]?[0-9]+)?)([upnfmM]?)M?',      # Kdå€¼  
            r'IC50=([0-9.]+(?:[eE][+-]?[0-9]+)?)([upnfmM]?)M?',    # IC50å€¼
        ]
        
        for pattern in patterns:
            match = re.search(pattern, affinity_str)
            if match:
                value = float(match.group(1))
                unit = match.group(2).lower() if match.group(2) else ''
                
                # æ ¹æ®å•ä½è½¬æ¢ä¸ºæ‘©å°”æµ“åº¦
                if unit == 'p':  # pM
                    value *= 1e-12
                elif unit == 'n':  # nM
                    value *= 1e-9
                elif unit == 'u':  # uM
                    value *= 1e-6
                elif unit == 'f':  # fM
                    value *= 1e-15
                elif unit == 'm':  # mM
                    value *= 1e-3
                else:  # æ— å•ä½æˆ–Mï¼Œå‡è®¾ä¸ºM
                    pass
                
                # è½¬æ¢ä¸º-log(Ka)
                # æ³¨æ„ï¼šå¯¹äºIC50ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼è½¬æ¢
                if value > 0:
                    return -math.log10(value)
        
        return None
    except Exception as e:
        print(f"Warning: Cannot parse affinity '{affinity_str}': {e}")
        return None

def load_affinity_data():
    """ä»INDEX_general_PL.2020æ–‡ä»¶åŠ è½½äº²å’ŒåŠ›æ•°æ®"""
    index_file = "./datasets/v2020-other-PL/v2020-other-PL/index/INDEX_general_PL.2020"
    affinity_data = {}
    
    print(f"Loading affinity data from {index_file}")
    
    with open(index_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split(None, 4)  # åˆ†å‰²ä¸ºæœ€å¤š5éƒ¨åˆ†
                if len(parts) >= 4:
                    pdb_id = parts[0].lower()
                    resolution = parts[1]
                    year = parts[2]
                    affinity_str = parts[3]
                    
                    affinity_value = parse_affinity_value(affinity_str)
                    if affinity_value is not None:
                        affinity_data[pdb_id] = affinity_value
    
    print(f"Loaded affinity data for {len(affinity_data)} complexes")
    return affinity_data

def get_v2020_other_pl_entries():
    """è·å–v2020-other-PLæ•°æ®é›†ä¸­æ‰€æœ‰æ¡ç›®"""
    v2020_dir = "./datasets/v2020-other-PL/v2020-other-PL"
    all_entries = []
    
    print(f"Scanning {v2020_dir} for entries...")
    for entry in os.listdir(v2020_dir):
        entry_path = os.path.join(v2020_dir, entry)
        if os.path.isdir(entry_path) and entry not in ['index', 'readme']:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶
            pdb_file = os.path.join(entry_path, f"{entry}_protein.pdb")
            sdf_file = os.path.join(entry_path, f"{entry}_ligand.sdf")
            
            if os.path.exists(pdb_file) and os.path.exists(sdf_file):
                all_entries.append(entry)
    
    print(f"Found {len(all_entries)} valid entries in v2020-other-PL")
    return all_entries

def parse_affinity_from_casf(pdb_id):
    """ä»CASF-2016/power_scoring/CoreSet.datè§£æäº²å’ŒåŠ›å€¼"""
    casf_file = "./datasets/v2020-other-PL/CASF-2016/power_scoring/CoreSet.dat"
    
    with open(casf_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 4 and parts[0].lower() == pdb_id.lower():
                    try:
                        log_ka = float(parts[3])
                        return log_ka
                    except ValueError:
                        print(f"Warning: Cannot parse logKa for {pdb_id}: {parts[3]}")
                        return None
    return None

def create_data_entry(pdb_id, affinity_value, data_split, is_casf=False):
    """åˆ›å»ºæ•°æ®æ¡ç›®"""
    if is_casf:
        # CASF-2016æ•°æ®è·¯å¾„
        return {
            'id': pdb_id,
            'affinity': {'neglog_aff': affinity_value},
            'protein_path': f"./datasets/v2020-other-PL/CASF-2016/coreset/{pdb_id}/{pdb_id}_protein.pdb",
            'ligand_path': f"./datasets/v2020-other-PL/CASF-2016/coreset/{pdb_id}/{pdb_id}_ligand.sdf",
            'split': data_split
        }
    else:
        # v2020-other-PLæ•°æ®è·¯å¾„
        return {
            'id': pdb_id,
            'affinity': {'neglog_aff': affinity_value},
            'protein_path': f"./datasets/v2020-other-PL/v2020-other-PL/{pdb_id}/{pdb_id}_protein.pdb",
            'ligand_path': f"./datasets/v2020-other-PL/v2020-other-PL/{pdb_id}/{pdb_id}_ligand.sdf",
            'split': data_split
        }

def main():
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "./datasets/v2020-other-PL/processed"
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½äº²å’ŒåŠ›æ•°æ®
    affinity_data = load_affinity_data()
    
    # è·å–CASF-2016æ¡ç›®
    casf_entries = get_casf_2016_entries()
    
    # è·å–æ‰€æœ‰v2020-other-PLæ¡ç›®
    all_v2020_entries = get_v2020_other_pl_entries()
    
    # æ‰¾å‡ºç”¨äºè®­ç»ƒçš„æ¡ç›®ï¼ˆv2020-other-PLä¸­é™¤å»CASF-2016çš„æ•°æ®ï¼‰
    train_valid_entries = []
    for entry in all_v2020_entries:
        if entry not in casf_entries and entry in affinity_data:
            train_valid_entries.append(entry)
    
    print(f"Training+Validation entries (v2020-other-PL excluding CASF-2016): {len(train_valid_entries)}")
    
    # éšæœºæ‰“ä¹±è®­ç»ƒ+éªŒè¯æ¡ç›®
    random.shuffle(train_valid_entries)
    
    # åˆ†å‰²ä¸ºè®­ç»ƒé›†(90%)å’ŒéªŒè¯é›†(10%)
    n_train_valid = len(train_valid_entries)
    n_valid = int(n_train_valid * 0.1)
    n_train = n_train_valid - n_valid
    
    train_entries = train_valid_entries[:n_train]
    valid_entries = train_valid_entries[n_train:]
    
    print(f"Training entries: {len(train_entries)}")
    print(f"Validation entries: {len(valid_entries)}")
    
    # å¤„ç†è®­ç»ƒé›†æ•°æ®
    train_data = []
    print("Processing training data...")
    for i, pdb_id in enumerate(train_entries):
        if i % 1000 == 0:
            print(f"  Processed {i}/{len(train_entries)} training entries")
        
        affinity = affinity_data[pdb_id]
        data_entry = create_data_entry(pdb_id, affinity, 'train', is_casf=False)
        train_data.append(data_entry)
    
    # å¤„ç†éªŒè¯é›†æ•°æ®
    valid_data = []
    print("Processing validation data...")
    for i, pdb_id in enumerate(valid_entries):
        if i % 1000 == 0:
            print(f"  Processed {i}/{len(valid_entries)} validation entries")
        
        affinity = affinity_data[pdb_id]
        data_entry = create_data_entry(pdb_id, affinity, 'valid', is_casf=False)
        valid_data.append(data_entry)
    
    # å¤„ç†æµ‹è¯•é›†æ•°æ®ï¼ˆCASF-2016æ‰€æœ‰285æ¡æ•°æ®ï¼‰
    test_data = []
    print("Processing test data (CASF-2016 coreset)...")
    for i, pdb_id in enumerate(sorted(casf_entries)):
        if i % 100 == 0:
            print(f"  Processed {i}/{len(casf_entries)} test entries")
        
        affinity = parse_affinity_from_casf(pdb_id)
        
        if affinity is not None:
            data_entry = create_data_entry(pdb_id, affinity, 'test', is_casf=True)
            test_data.append(data_entry)
        else:
            print(f"Warning: No affinity found for test entry {pdb_id}")
    
    # ä¿å­˜æ•°æ®
    train_file = os.path.join(output_dir, "train.pkl")
    valid_file = os.path.join(output_dir, "valid.pkl")
    test_file = os.path.join(output_dir, "test.pkl")
    
    print(f"\nSaving data...")
    print(f"Training set: {len(train_data)} entries -> {train_file}")
    print(f"Validation set: {len(valid_data)} entries -> {valid_file}")
    print(f"Test set: {len(test_data)} entries -> {test_file}")
    
    with open(train_file, 'wb') as f:
        pickle.dump(train_data, f)
    
    with open(valid_file, 'wb') as f:
        pickle.dump(valid_data, f)
    
    with open(test_file, 'wb') as f:
        pickle.dump(test_data, f)
    
    print(f"\nâœ… Data processing completed successfully!")
    print(f"ğŸ“Š Dataset statistics:")
    print(f"   Training: {len(train_data)} complexes (90% of v2020-other-PL)")
    print(f"   Validation: {len(valid_data)} complexes (10% of v2020-other-PL)")
    print(f"   Test: {len(test_data)} complexes (CASF-2016 coreset)")
    print(f"   Total: {len(train_data) + len(valid_data) + len(test_data)} complexes")
    
    # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    stats_file = os.path.join(output_dir, "dataset_stats.txt")
    with open(stats_file, 'w') as f:
        f.write("v2020-other-PL Dataset Statistics (Final)\n")
        f.write("==========================================\n")
        f.write(f"Training set: {len(train_data)} complexes (90% of v2020-other-PL excluding CASF-2016)\n")
        f.write(f"Validation set: {len(valid_data)} complexes (10% of v2020-other-PL excluding CASF-2016)\n")
        f.write(f"Test set: {len(test_data)} complexes (CASF-2016 coreset)\n")
        f.write(f"Total: {len(train_data) + len(valid_data) + len(test_data)} complexes\n")
        f.write(f"\nCASF-2016 total entries: {len(casf_entries)}\n")
        f.write(f"CASF-2016 entries with affinity data: {len(test_data)}\n")
        f.write(f"v2020-other-PL total entries: {len(all_v2020_entries)}\n")
        f.write(f"v2020-other-PL entries with affinity data: {len(affinity_data)}\n")

if __name__ == "__main__":
    main()
