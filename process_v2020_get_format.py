#!/usr/bin/env python3
"""
æŒ‰ç…§GETå¤„ç†PDBBindçš„æ–¹å¼å¤„ç†v2020-other-PL + CASF-2016æ•°æ®é›†
"""

import os
import sys
import json
import pickle
import argparse
import random
import re
import math
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJ_DIR = os.path.join(
    os.path.split(os.path.abspath(__file__))[0],
    '..'
)
print(f'Project directory: {PROJ_DIR}')
sys.path.append(PROJ_DIR)

from utils.logger import print_log
from data.pdb_utils import Residue, VOCAB
from data.dataset import blocks_interface, blocks_to_data
from data.converter.pdb_to_list_blocks import pdb_to_list_blocks
from data.converter.mol2_to_blocks import mol2_to_blocks
# from data.converter.sdf_to_blocks import sdf_to_blocks  # ä¸å­˜åœ¨ï¼Œæ”¹ç”¨mol2_to_blocks

def get_casf_2016_entries():
    """ä»CASF-2016/power_scoring/CoreSet.datè·å–æ‰€æœ‰æ¡ç›®"""
    casf_file = "./datasets/v2020-other-PL/CASF-2016/power_scoring/CoreSet.dat"
    casf_entries = set()
    
    print(f"Reading CASF-2016 entries from {casf_file}")
    with open(casf_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 1:
                    pdb_id = parts[0].lower()  # è½¬æ¢ä¸ºå°å†™ä»¥åŒ¹é…ç›®å½•å
                    casf_entries.add(pdb_id)
    
    print(f"Found {len(casf_entries)} CASF-2016 entries")
    return casf_entries

def parse_affinity_value(affinity_str):
    """è§£æäº²å’ŒåŠ›å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸º-log(Ka)å€¼ï¼Œåªä¿ç•™ç¡®å®šçš„å€¼"""
    try:
        # å»é™¤æ³¨é‡Šéƒ¨åˆ†
        if '//' in affinity_str:
            affinity_str = affinity_str.split('//')[0].strip()
        
        # è·³è¿‡æ‰€æœ‰ä¸ç¡®å®šçš„å€¼ï¼ˆåŒ…å«ä¸ç­‰å·æˆ–çº¦ç­‰å·ï¼‰
        if any(symbol in affinity_str for symbol in ['<', '>', '~', '<=', '>=']):
            return None
        
        # æ›´å¹¿æ³›çš„æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…Ki, Kd, IC50
        patterns = [
            r'Ki=([0-9.]+(?:[eE][+-]?[0-9]+)?)([upnfmM]?)M?',      # Kiå€¼
            r'Kd=([0-9.]+(?:[eE][+-]?[0-9]+)?)([upnfmM]?)M?',      # Kdå€¼  
            r'IC50=([0-9.]+(?:[eE][+-]?[0-9]+)?)([upnfmM]?)M?',    # IC50å€¼
        ]
        
        for pattern in patterns:
            match = re.search(pattern, affinity_str)
            if match:
                value = float(match.group(1))
                unit = match.group(2).lower() if match.group(2) else ''
                
                # æ ¹æ®å•ä½è½¬æ¢ä¸ºæ‘©å°”æµ“åº¦
                if unit == 'p':  # pM
                    value *= 1e-12
                elif unit == 'n':  # nM
                    value *= 1e-9
                elif unit == 'u':  # uM
                    value *= 1e-6
                elif unit == 'f':  # fM
                    value *= 1e-15
                elif unit == 'm':  # mM
                    value *= 1e-3
                else:  # æ— å•ä½æˆ–Mï¼Œå‡è®¾ä¸ºM
                    pass
                
                # è½¬æ¢ä¸º-log(Ka)
                if value > 0:
                    return -math.log10(value)
        
        return None
    except Exception as e:
        print(f"Warning: Cannot parse affinity '{affinity_str}': {e}")
        return None

def load_affinity_data():
    """ä»INDEX_general_PL.2020æ–‡ä»¶åŠ è½½äº²å’ŒåŠ›æ•°æ®"""
    index_file = "./datasets/v2020-other-PL/v2020-other-PL/index/INDEX_general_PL.2020"
    affinity_data = {}
    
    print(f"Loading affinity data from {index_file}")
    
    with open(index_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split(None, 4)  # åˆ†å‰²ä¸ºæœ€å¤š5éƒ¨åˆ†
                if len(parts) >= 4:
                    pdb_id = parts[0].lower()
                    affinity_str = parts[3]
                    
                    affinity_value = parse_affinity_value(affinity_str)
                    if affinity_value is not None:
                        affinity_data[pdb_id] = affinity_value
    
    print(f"Loaded affinity data for {len(affinity_data)} complexes")
    return affinity_data

def parse_affinity_from_casf(pdb_id):
    """ä»CASF-2016/power_scoring/CoreSet.datè§£æäº²å’ŒåŠ›å€¼"""
    casf_file = "./datasets/v2020-other-PL/CASF-2016/power_scoring/CoreSet.dat"
    
    with open(casf_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                parts = line.split()
                if len(parts) >= 4 and parts[0].lower() == pdb_id.lower():
                    try:
                        log_ka = float(parts[3])
                        return log_ka
                    except ValueError:
                        print(f"Warning: Cannot parse logKa for {pdb_id}: {parts[3]}")
                        return None
    return None

def process_one_v2020(pdb_id, label, interface_dist_th=8.0):
    """å¤„ç†ä¸€ä¸ªv2020-other-PLæ¡ç›®"""
    
    item = {}
    item['id'] = pdb_id
    item['affinity'] = {'neglog_aff': label}
    
    # v2020-other-PLæ–‡ä»¶è·¯å¾„
    prot_fname = f"./datasets/v2020-other-PL/v2020-other-PL/{pdb_id}/{pdb_id}_protein.pdb"
    ligand_fname = f"./datasets/v2020-other-PL/v2020-other-PL/{pdb_id}/{pdb_id}_ligand.mol2"
    
    try:
        list_blocks1 = pdb_to_list_blocks(prot_fname)
    except Exception as e:
        print_log(f'{pdb_id} protein parsing failed: {e}', level='ERROR')
        return None
    
    try:
        blocks2 = mol2_to_blocks(ligand_fname)
    except Exception as e:
        print_log(f'{pdb_id} ligand parsing failed: {e}', level='ERROR')
        return None
    
    blocks1 = []
    for b in list_blocks1:
        blocks1.extend(b)
    
    # æ„å»ºæ¥å£
    blocks1, _ = blocks_interface(blocks1, blocks2, interface_dist_th)
    if len(blocks1) == 0:
        print_log(f'{pdb_id} has no interface', level='ERROR')
        return None
    
    data = blocks_to_data(blocks1, blocks2)
    for key in data:
        if hasattr(data[key], 'tolist'):  # numpy array
            data[key] = data[key].tolist()
    
    item['data'] = data
    return item

def process_one_casf(pdb_id, label, interface_dist_th=8.0):
    """å¤„ç†ä¸€ä¸ªCASF-2016æ¡ç›®"""
    
    item = {}
    item['id'] = pdb_id
    item['affinity'] = {'neglog_aff': label}
    
    # CASF-2016æ–‡ä»¶è·¯å¾„
    prot_fname = f"./datasets/v2020-other-PL/CASF-2016/coreset/{pdb_id}/{pdb_id}_protein.pdb"
    ligand_fname = f"./datasets/v2020-other-PL/CASF-2016/coreset/{pdb_id}/{pdb_id}_ligand.mol2"
    
    try:
        list_blocks1 = pdb_to_list_blocks(prot_fname)
    except Exception as e:
        print_log(f'{pdb_id} protein parsing failed: {e}', level='ERROR')
        return None
    
    try:
        blocks2 = mol2_to_blocks(ligand_fname)
    except Exception as e:
        print_log(f'{pdb_id} ligand parsing failed: {e}', level='ERROR')
        return None
    
    blocks1 = []
    for b in list_blocks1:
        blocks1.extend(b)
    
    # æ„å»ºæ¥å£
    blocks1, _ = blocks_interface(blocks1, blocks2, interface_dist_th)
    if len(blocks1) == 0:
        print_log(f'{pdb_id} has no interface', level='ERROR')
        return None
    
    data = blocks_to_data(blocks1, blocks2)
    for key in data:
        if hasattr(data[key], 'tolist'):  # numpy array
            data[key] = data[key].tolist()
    
    item['data'] = data
    return item

def main():
    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "./datasets/v2020-other-PL/processed_get_format"
    os.makedirs(output_dir, exist_ok=True)
    
    print_log("å¼€å§‹å¤„ç†v2020-other-PL + CASF-2016æ•°æ®é›†")
    
    # åŠ è½½äº²å’ŒåŠ›æ•°æ®
    affinity_data = load_affinity_data()
    
    # è·å–CASF-2016æ¡ç›®
    casf_entries = get_casf_2016_entries()
    
    # è·å–æ‰€æœ‰v2020-other-PLæ¡ç›®
    v2020_dir = "./datasets/v2020-other-PL/v2020-other-PL"
    all_v2020_entries = []
    
    print(f"Scanning {v2020_dir} for entries...")
    for entry in os.listdir(v2020_dir):
        entry_path = os.path.join(v2020_dir, entry)
        if os.path.isdir(entry_path) and entry not in ['index', 'readme']:
            pdb_file = os.path.join(entry_path, f"{entry}_protein.pdb")
            sdf_file = os.path.join(entry_path, f"{entry}_ligand.sdf")
            
            if os.path.exists(pdb_file) and os.path.exists(sdf_file):
                all_v2020_entries.append(entry)
    
    print(f"Found {len(all_v2020_entries)} valid entries in v2020-other-PL")
    
    # æ‰¾å‡ºç”¨äºè®­ç»ƒ+éªŒè¯çš„æ¡ç›®ï¼ˆv2020-other-PLä¸­é™¤å»CASF-2016ä¸”æœ‰äº²å’ŒåŠ›æ•°æ®çš„ï¼‰
    train_valid_entries = []
    for entry in all_v2020_entries:
        if entry not in casf_entries and entry in affinity_data:
            train_valid_entries.append(entry)
    
    print(f"Training+Validation entries: {len(train_valid_entries)}")
    
    # éšæœºæ‰“ä¹±å¹¶åˆ†å‰²
    random.shuffle(train_valid_entries)
    n_train_valid = len(train_valid_entries)
    n_valid = int(n_train_valid * 0.1)
    n_train = n_train_valid - n_valid
    
    train_entries = train_valid_entries[:n_train]
    valid_entries = train_valid_entries[n_train:]
    
    print(f"Training entries: {len(train_entries)}")
    print(f"Validation entries: {len(valid_entries)}")
    
    # å¤„ç†è®­ç»ƒé›†
    print_log("Processing training data...")
    train_data = []
    for i, pdb_id in enumerate(train_entries):
        if i % 1000 == 0:
            print_log(f"Processed {i}/{len(train_entries)} training entries")
        
        label = affinity_data[pdb_id]
        item = process_one_v2020(pdb_id, label)
        if item is not None:
            train_data.append(item)
        else:
            print_log(f"Failed to process training entry: {pdb_id}")
    
    # å¤„ç†éªŒè¯é›†
    print_log("Processing validation data...")
    valid_data = []
    for i, pdb_id in enumerate(valid_entries):
        if i % 1000 == 0:
            print_log(f"Processed {i}/{len(valid_entries)} validation entries")
        
        label = affinity_data[pdb_id]
        item = process_one_v2020(pdb_id, label)
        if item is not None:
            valid_data.append(item)
        else:
            print_log(f"Failed to process validation entry: {pdb_id}")
    
    # å¤„ç†æµ‹è¯•é›†ï¼ˆCASF-2016ï¼‰
    print_log("Processing test data (CASF-2016 coreset)...")
    test_data = []
    for i, pdb_id in enumerate(sorted(casf_entries)):
        if i % 100 == 0:
            print_log(f"Processed {i}/{len(casf_entries)} test entries")
        
        label = parse_affinity_from_casf(pdb_id)
        if label is not None:
            item = process_one_casf(pdb_id, label)
            if item is not None:
                test_data.append(item)
            else:
                print_log(f"Failed to process test entry: {pdb_id}")
        else:
            print_log(f"No affinity found for test entry: {pdb_id}")
    
    # ä¿å­˜æ•°æ®
    train_file = os.path.join(output_dir, "train.pkl")
    valid_file = os.path.join(output_dir, "valid.pkl")
    test_file = os.path.join(output_dir, "test.pkl")
    
    print_log(f"Saving training data: {len(train_data)} entries -> {train_file}")
    with open(train_file, 'wb') as f:
        pickle.dump(train_data, f)
    
    print_log(f"Saving validation data: {len(valid_data)} entries -> {valid_file}")
    with open(valid_file, 'wb') as f:
        pickle.dump(valid_data, f)
    
    print_log(f"Saving test data: {len(test_data)} entries -> {test_file}")
    with open(test_file, 'wb') as f:
        pickle.dump(test_data, f)
    
    print_log("âœ… Data processing completed successfully!")
    print_log(f"ğŸ“Š Dataset statistics:")
    print_log(f"   Training: {len(train_data)} complexes")
    print_log(f"   Validation: {len(valid_data)} complexes")
    print_log(f"   Test: {len(test_data)} complexes")
    print_log(f"   Total: {len(train_data) + len(valid_data) + len(test_data)} complexes")

if __name__ == "__main__":
    main()
